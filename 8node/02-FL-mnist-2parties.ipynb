{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a remote Deep Learning model\n",
    "In this notebbok, we will show how to train a Federated Deep Learning with data hosted in Nodes.\n",
    "\n",
    "We will consider that you are a Data Scientist and you do not know where data lives, you only have access to GridNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Previous setup\n",
    "\n",
    "Components:\n",
    "\n",
    " - PyGrid Network      203.145.218.196:80\n",
    " - PyGrid Node Alice ( http://alice.libthomas.org:80)\n",
    " - PyGrid Node Bob   (http://bob.libthomas.org:80)\n",
    "\n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Here we import core dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/opt/conda/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.3.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from syft.federated.floptimizer import Optims\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syft and client configuration\n",
    "Now we hook Torch and connect to the GridNetwork. This is the only sever you do not need to know node addresses (networks knows), but lets first define some useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "grid_address = \"http://203.145.221.20:80\"  # address\n",
    "N_EPOCHS = 100# number of epochs to train\n",
    "N_TEST   = 128   # number of test\n",
    "parties = 4\n",
    "TAG_NAME = str(parties)+\"data_15000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "\n",
    "# Connect direcly to grid nodes\n",
    "my_grid = PublicGridNetwork(hook, grid_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Define our Neural Network Arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a Deep Learning Network, feel free to write your own model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.test_batch_size = N_TEST\n",
    "        self.epochs = N_EPOCHS\n",
    "        self.lr = 0.01\n",
    "        self.log_interval = 5\n",
    "        #self.device = th.device(\"cpu\")\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(5408, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.conv1(x)        \n",
    "        x = F.relu(x)        \n",
    "        #x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)        \n",
    "        #x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)        \n",
    "        x = self.fc1(x)        \n",
    "        x = F.relu(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "#         self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "#         self.fc1 = nn.Linear(5*5*50, 500)\n",
    "#         self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "       \n",
    "#         x = F.max_pool2d(x, 8, 8)       \n",
    "#         x = F.relu(self.conv1(x))        \n",
    "#         x = F.max_pool2d(x, 2, 2)        \n",
    "#         x = F.relu(self.conv2(x))        \n",
    "#         x = F.max_pool2d(x, 2, 2)        \n",
    "#         #x = x.view(-1, 4*4*50)\n",
    "#         x = x.view(-1, 5*5*50)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "#         self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "#         self.fc1 = nn.Linear(4*4*50, 500)\n",
    "#         self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, 2, 2)\n",
    "#         x = x.view(-1, 4*4*50)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=[th.device(\"cuda:2\"),th.device(\"cuda:3\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=5408, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if(torch.cuda.is_available()):\n",
    "#     torch.set_default_tensor_type(th.cuda.FloatTensor)\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name = [\"gridnode01\",\"gridnode02\",\"gridnode03\",\"gridnode04\",\"gridnode05\",\"gridnode06\",\"gridnode07\",\"gridnode08\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.federated.floptimizer import Optims\n",
    "\n",
    "workers =node_name[:parties]\n",
    "optims = Optims(workers, optim=optim.Adam(params=model.parameters(),lr=args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Search for remote data\n",
    "\n",
    "Once we have defined our Deep Learning Network, we need some data to train... Thanks to PyGridNetwork this is very easy, you just need to search for your tags of interest.\n",
    "\n",
    "Notice that _search()_ method  returns a pointer tensor, so we will work with those keeping real tensors hosted in Alice and Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_grid.search(\"#X_\"+TAG_NAME)  # images\n",
    "target = my_grid.search(\"#Y_\"+TAG_NAME)  # labels\n",
    "\n",
    "data = list(data.values())  # returns a pointer\n",
    "target = list(target.values())  # returns a pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4data_15000'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the tensors, we can check how the metadata we added before is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(Wrapper)>[PointerTensor | me:62199280257 -> gridnode01:96162323071]\n",
      "\tTags: #X_4data_15000 \n",
      "\tShape: torch.Size([15000, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 4 parties...], [(Wrapper)>[PointerTensor | me:25126542397 -> gridnode02:84830398178]\n",
      "\tTags: #X_4data_15000 \n",
      "\tShape: torch.Size([15000, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 4 parties...], [(Wrapper)>[PointerTensor | me:76189664482 -> gridnode03:76103661941]\n",
      "\tTags: #X_4data_15000 \n",
      "\tShape: torch.Size([15000, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 4 parties...], [(Wrapper)>[PointerTensor | me:63362693488 -> gridnode04:66835842299]\n",
      "\tTags: #X_4data_15000 \n",
      "\tShape: torch.Size([15000, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 4 parties...]]\n",
      "[[(Wrapper)>[PointerTensor | me:9628530458 -> gridnode01:63590306640]\n",
      "\tTags: #Y_4data_15000 \n",
      "\tShape: torch.Size([15000])\n",
      "\tDescription: input mnist labels split 4 parties...], [(Wrapper)>[PointerTensor | me:56125697999 -> gridnode02:95471138570]\n",
      "\tTags: #Y_4data_15000 \n",
      "\tShape: torch.Size([15000])\n",
      "\tDescription: input mnist labels split 4 parties...], [(Wrapper)>[PointerTensor | me:69316932129 -> gridnode03:10797645185]\n",
      "\tTags: #Y_4data_15000 \n",
      "\tShape: torch.Size([15000])\n",
      "\tDescription: input mnist labels split 4 parties...], [(Wrapper)>[PointerTensor | me:90156410613 -> gridnode04:16908229242]\n",
      "\tTags: #Y_4data_15000 \n",
      "\tShape: torch.Size([15000])\n",
      "\tDescription: input mnist labels split 4 parties...]]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:3610306298 -> gridnode01:51937680995]\n",
       "\tTags: #X_1data_60000 \n",
       "\tShape: torch.Size([30000, 1, 28, 28])\n",
       "\tDescription: input mnist datapoinsts split 1 parties..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fffb1f4fda32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "worker = data[1][0].location\n",
    "worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. As you will see, this is very similar to standard pytorch sintax.\n",
    "\n",
    "Let's first load test data in order to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import read_mnist_data\n",
    "BATCH_SIZE = 128\n",
    "# train_loader_x = []\n",
    "# train_loader_y = []\n",
    "\n",
    "# parties = 2\n",
    "# for idx in range(parties): \n",
    "npz_path = '../'+str(parties)+'Parties/data_party0.npz'\n",
    "mnist_train_loader,mnist_test_loader = read_mnist_data(npz_path, batch = BATCH_SIZE )\n",
    "    \n",
    "    \n",
    "#     dataiter = iter(mnist_train_loader)\n",
    "#     images_train_mnist, labels_train_mnist = dataiter.next()\n",
    "    \n",
    "    \n",
    "#     images_train_mnist = images_train_mnist.to(device)\n",
    "#     labels_train_mnist = labels_train_mnist.to(device)\n",
    "    \n",
    "#     train_loader_x.append(images_train_mnist)\n",
    "#     train_loader_y.append(labels_train_mnist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch size\n",
    "def epoch_total_size(data):\n",
    "    total = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            total += data[i][j].shape[0]\n",
    "            \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0 : 15000 15000\n",
      "1, 0 : 15000 15000\n",
      "2, 0 : 15000 15000\n",
      "3, 0 : 15000 15000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        print(\"{}, {} : {} {}\".format(i,j, len(data[i][j]), len(target[i][j])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       \n",
    "            \n",
    "def train(args):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_total = epoch_total_size(data)    \n",
    "    current_epoch_size = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            \n",
    "            current_epoch_size += len(data[i][j])\n",
    "            worker = data[i][j].location  # worker hosts data\n",
    "            \n",
    "            model.send(worker)  # send model to PyGridNode worker\n",
    "            \n",
    "            optimizer = optims.get_optim(worker.id)\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            \n",
    "            pred = model(data[i][j])\n",
    "            #print(pred)\n",
    "            loss = F.nll_loss(pred, target[i][j])\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.get()  # get back the model\n",
    "            \n",
    "            loss = loss.get()\n",
    "            \n",
    "        if epoch % args.log_interval == 0:\n",
    "\n",
    "            print('Train Epoch: {} | With {} data |: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, worker.id, current_epoch_size, epoch_total,\n",
    "                            100. *  current_epoch_size / epoch_total, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(args,fo,train_time):\n",
    "    \n",
    "    if epoch % args.log_interval == 0 :\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in mnist_test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(mnist_test_loader.dataset)\n",
    "        \n",
    "        fo.write(\"{},{:.4f},{:.0f},{:.4f}\\n\".format(epoch, test_loss,100. * correct / len(mnist_test_loader.dataset),train_time))\n",
    "\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(mnist_test_loader.dataset),\n",
    "            100. * correct / len(mnist_test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 2.304160\n",
      "Train Epoch: 0 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 7.011192\n",
      "Train Epoch: 0 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 8.165142\n",
      "Train Epoch: 0 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 8.443104\n",
      "\n",
      "Test set: Average loss: 10.6651, Accuracy: 265/2500 (11%)\n",
      "\n",
      "Train Epoch: 5 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 1.140459\n",
      "Train Epoch: 5 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 1.113482\n",
      "Train Epoch: 5 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 1.034042\n",
      "Train Epoch: 5 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 1.044780\n",
      "\n",
      "Test set: Average loss: 0.8745, Accuracy: 1799/2500 (72%)\n",
      "\n",
      "Train Epoch: 10 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.647384\n",
      "Train Epoch: 10 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.458505\n",
      "Train Epoch: 10 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.539062\n",
      "Train Epoch: 10 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.393083\n",
      "\n",
      "Test set: Average loss: 0.5199, Accuracy: 2076/2500 (83%)\n",
      "\n",
      "Train Epoch: 15 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.265250\n",
      "Train Epoch: 15 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.311190\n",
      "Train Epoch: 15 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.296091\n",
      "Train Epoch: 15 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.320779\n",
      "\n",
      "Test set: Average loss: 0.2713, Accuracy: 2306/2500 (92%)\n",
      "\n",
      "Train Epoch: 20 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.217709\n",
      "Train Epoch: 20 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.179188\n",
      "Train Epoch: 20 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.239575\n",
      "Train Epoch: 20 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.220322\n",
      "\n",
      "Test set: Average loss: 0.2495, Accuracy: 2310/2500 (92%)\n",
      "\n",
      "Train Epoch: 25 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.194836\n",
      "Train Epoch: 25 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.166109\n",
      "Train Epoch: 25 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.163420\n",
      "Train Epoch: 25 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.226972\n",
      "\n",
      "Test set: Average loss: 0.2421, Accuracy: 2309/2500 (92%)\n",
      "\n",
      "Train Epoch: 30 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.205723\n",
      "Train Epoch: 30 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.167411\n",
      "Train Epoch: 30 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.146318\n",
      "Train Epoch: 30 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.217647\n",
      "\n",
      "Test set: Average loss: 0.2774, Accuracy: 2287/2500 (91%)\n",
      "\n",
      "Train Epoch: 35 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.227512\n",
      "Train Epoch: 35 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.178897\n",
      "Train Epoch: 35 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.155964\n",
      "Train Epoch: 35 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.205223\n",
      "\n",
      "Test set: Average loss: 0.3141, Accuracy: 2284/2500 (91%)\n",
      "\n",
      "Train Epoch: 40 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.210637\n",
      "Train Epoch: 40 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.178951\n",
      "Train Epoch: 40 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.162974\n",
      "Train Epoch: 40 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.185526\n",
      "\n",
      "Test set: Average loss: 0.2887, Accuracy: 2291/2500 (92%)\n",
      "\n",
      "Train Epoch: 45 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.186377\n",
      "Train Epoch: 45 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.179446\n",
      "Train Epoch: 45 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.167766\n",
      "Train Epoch: 45 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.186934\n",
      "\n",
      "Test set: Average loss: 0.2550, Accuracy: 2305/2500 (92%)\n",
      "\n",
      "Train Epoch: 50 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.182861\n",
      "Train Epoch: 50 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.168558\n",
      "Train Epoch: 50 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.181322\n",
      "Train Epoch: 50 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.193880\n",
      "\n",
      "Test set: Average loss: 0.2604, Accuracy: 2305/2500 (92%)\n",
      "\n",
      "Train Epoch: 55 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.196175\n",
      "Train Epoch: 55 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.164321\n",
      "Train Epoch: 55 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.182917\n",
      "Train Epoch: 55 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.206238\n",
      "\n",
      "Test set: Average loss: 0.2768, Accuracy: 2310/2500 (92%)\n",
      "\n",
      "Train Epoch: 60 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.207593\n",
      "Train Epoch: 60 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.173825\n",
      "Train Epoch: 60 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.171707\n",
      "Train Epoch: 60 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.234455\n",
      "\n",
      "Test set: Average loss: 0.3225, Accuracy: 2293/2500 (92%)\n",
      "\n",
      "Train Epoch: 65 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.256390\n",
      "Train Epoch: 65 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.180886\n",
      "Train Epoch: 65 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.167997\n",
      "Train Epoch: 65 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.248203\n",
      "\n",
      "Test set: Average loss: 0.4159, Accuracy: 2251/2500 (90%)\n",
      "\n",
      "Train Epoch: 70 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.302354\n",
      "Train Epoch: 70 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.200837\n",
      "Train Epoch: 70 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.193085\n",
      "Train Epoch: 70 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.227859\n",
      "\n",
      "Test set: Average loss: 0.4383, Accuracy: 2235/2500 (89%)\n",
      "\n",
      "Train Epoch: 75 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.233538\n",
      "Train Epoch: 75 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.204489\n",
      "Train Epoch: 75 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.188830\n",
      "Train Epoch: 75 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.241455\n",
      "\n",
      "Test set: Average loss: 0.3574, Accuracy: 2270/2500 (91%)\n",
      "\n",
      "Train Epoch: 80 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.205222\n",
      "Train Epoch: 80 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.195134\n",
      "Train Epoch: 80 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.188249\n",
      "Train Epoch: 80 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.252432\n",
      "\n",
      "Test set: Average loss: 0.3051, Accuracy: 2299/2500 (92%)\n",
      "\n",
      "Train Epoch: 85 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.247528\n",
      "Train Epoch: 85 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.187832\n",
      "Train Epoch: 85 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.223983\n",
      "Train Epoch: 85 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.265449\n",
      "\n",
      "Test set: Average loss: 0.3335, Accuracy: 2308/2500 (92%)\n",
      "\n",
      "Train Epoch: 90 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.559475\n",
      "Train Epoch: 90 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.217258\n",
      "Train Epoch: 90 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.281104\n",
      "Train Epoch: 90 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.353860\n",
      "\n",
      "Test set: Average loss: 0.7345, Accuracy: 2188/2500 (88%)\n",
      "\n",
      "Train Epoch: 95 | With gridnode01 data |: [15000/60000 (25%)]\tLoss: 0.414673\n",
      "Train Epoch: 95 | With gridnode02 data |: [30000/60000 (50%)]\tLoss: 0.376331\n",
      "Train Epoch: 95 | With gridnode03 data |: [45000/60000 (75%)]\tLoss: 0.292758\n",
      "Train Epoch: 95 | With gridnode04 data |: [60000/60000 (100%)]\tLoss: 0.441695\n",
      "\n",
      "Test set: Average loss: 0.6225, Accuracy: 2226/2500 (89%)\n",
      "\n",
      "CPU times: user 38 s, sys: 9.5 s, total: 47.5 s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA) \n",
    "output_file_name = TAG_NAME+'.csv'\n",
    "fo = open(\"output/\"+output_file_name, \"w\")\n",
    "\n",
    "\n",
    "\n",
    "last_time = time.time()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train(args)\n",
    "    if epoch % args.log_interval == 0 :\n",
    "        train_time = time.time()-last_time\n",
    "        #last_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    test(args,fo,train_time)\n",
    "    \n",
    "\n",
    "    #scheduler.step()\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_train_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ea86c7e69e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_train_hist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't_train_hist' is not defined"
     ]
    }
   ],
   "source": [
    "t_train_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voil√†! Here you are, you have trained a model on remote data using Federated Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PyGrid on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PyGrid](https://github.com/OpenMined/PyGrid)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PyGrid/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
