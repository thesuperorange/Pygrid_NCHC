{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate remote GridNodes with labeled tensors\n",
    "In this notebbok, we will show how to populate a GridNode with labeled data, so it will be used later (link to second part) by people interested in train models.\n",
    "\n",
    "In particular, we will consider that two Data Owners (Alice & Bob) want to populate their nodes with some data from the well-known MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Previous setup\n",
    "\n",
    "Components:\n",
    "\n",
    " - PyGrid Network      203.145.218.196:80\n",
    " - PyGrid Node Alice ( http://alice.libthomas.org:80)\n",
    " - PyGrid Node Bob   (http://bob.libthomas.org:80)\n",
    "\n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Here we import core dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/opt/conda/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.3.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient  # websocket client. It sends commands to the node servers\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syft and client configuration\n",
    "Now we hook Torch and connect the clients to the servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "parties = 4\n",
    "N_SAMPLES = 15000  # Number of samples\n",
    "TAG_NAME = str(parties)+\"data_15000\"\n",
    "# address\n",
    "\n",
    "gridnode01 = \"http://203.145.219.187:53980\"\n",
    "gridnode02 = \"http://203.145.219.187:53946\"\n",
    "gridnode03 = \"http://203.145.219.187:53359\"\n",
    "gridnode04 = \"http://203.145.219.187:56716\"\n",
    "gridnode05 = \"http://203.145.219.187:57096\"\n",
    "gridnode06 = \"http://203.145.219.187:55194\"\n",
    "gridnode07 = \"http://203.145.219.187:57574\"\n",
    "gridnode08 = \"http://203.145.219.187:52228\"\n",
    "address_list = [gridnode01,gridnode02,gridnode03,gridnode04,gridnode05,gridnode06,gridnode07,gridnode08]        \n",
    "node_name = [\"gridnode01\",\"gridnode02\",\"gridnode03\",\"gridnode04\",\"gridnode05\",\"gridnode06\",\"gridnode07\",\"gridnode08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is gridnode01 connected?: True\n",
      "Is gridnode02 connected?: True\n",
      "Is gridnode03 connected?: True\n",
      "Is gridnode04 connected?: True\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "\n",
    "\n",
    "# Connect direcly to grid nodes\n",
    "compute_nodes = {}\n",
    "for idx in range(parties): \n",
    "    compute_nodes[node_name[idx]] = DataCentricFLClient(hook, address_list[idx])\n",
    "\n",
    "\n",
    "# Check if they are connected\n",
    "for key, value in compute_nodes.items(): \n",
    "    print(\"Is \" + key + \" connected?: \" + str(value.ws.connected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load dataset\n",
    "Download (and load) the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=[torch.device(\"cuda:2\"),torch.device(\"cuda:3\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import read_mnist_data\n",
    "\n",
    "# dataset_path = 'Data'\n",
    "# # Define a transformation.\n",
    "# transform = transforms.Compose([\n",
    "#                               transforms.ToTensor(),\n",
    "#                               transforms.Normalize((0.1307,), (0.3081,)),  #  mean and std \n",
    "#                               ])\n",
    "\n",
    "# trainset = NpcPatchDataset(train=True, root='Data', transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=N_SAMPLES, shuffle=True)\n",
    "\n",
    "\n",
    "train_loader_x = []\n",
    "train_loader_y = []\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(parties): \n",
    "    npz_path = '../'+str(parties)+'Parties/data_party'+str(idx)+'.npz'\n",
    "    mnist_train_loader,mnist_test_loader = read_mnist_data(npz_path, batch = N_SAMPLES )\n",
    "    \n",
    "    dataiter = iter(mnist_train_loader)\n",
    "    images_train_mnist, labels_train_mnist = dataiter.next()\n",
    "    \n",
    "    \n",
    "    images_train_mnist = images_train_mnist.to(device)\n",
    "    labels_train_mnist = labels_train_mnist.to(device)\n",
    "    \n",
    "    train_loader_x.append(images_train_mnist)\n",
    "    train_loader_y.append(labels_train_mnist)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Split dataset\n",
    "We split our dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parties = 2\n",
    "#for index, _ in enumerate(parties):\n",
    "for index in range(parties): \n",
    "    \n",
    "    train_loader_x[index].tag(\"#X_\"+TAG_NAME)\\\n",
    "        .describe(\"input mnist datapoinsts split \" +str(parties)+ \" parties\")\n",
    "    train_loader_y[index].tag(\"#Y_\"+TAG_NAME)\\\n",
    "        .describe(\"input mnist labels split \" +str(parties)+ \" parties\")\n",
    "#     images_train_mnist[index]\\\n",
    "#         .tag(\"#X\", \"#npc_100_dynamic_cuda\", \"#dataset\")\\\n",
    "#         .describe(\"The input datapoints to the MNIST dataset.\") \n",
    "    \n",
    "    \n",
    "#     labels_train_mnist[index]\\\n",
    "#         .tag(\"#Y\", \"#npc_100_dynamic_cuda\", \"#dataset\") \\\n",
    "#         .describe(\"The input labels to the MNIST dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Sending our tensor to grid nodes\n",
    "\n",
    "We can consider the previous steps as data preparation, i.e., in a more realistic scenario Alice and Bob would already have their data, so they just would need to load their tensors into their nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sending data to gridnode01\n",
      "1\n",
      "Sending data to gridnode02\n",
      "2\n",
      "Sending data to gridnode03\n",
      "3\n",
      "Sending data to gridnode04\n"
     ]
    }
   ],
   "source": [
    "for index in range(parties):\n",
    "    print(index)\n",
    "    \n",
    "    \n",
    "    print(\"Sending data to {}\".format( node_name[index]))\n",
    "    train_loader_x[index].send(compute_nodes[node_name[index]], garbage_collect_data=False)\n",
    "    train_loader_y[index].send(compute_nodes[node_name[index]], garbage_collect_data=False)\n",
    "#     images_train_mnist[index].send(compute_nodes[key], garbage_collect_data=False)\n",
    "#     labels_train_mnist[index].send(compute_nodes[key], garbage_collect_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is ok, tensors must be hosted in the nodes. GridNode have a specific endpoint to request what tensors are hosted. Let's check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridnode01's tags:  ['#Y_1data', '#Y_2data', '#X_2data', '#Y_8data', '#Y_4data', '#X_4data_15000', '#X_1data', '#Y_1data_60000', '#Y_2data_30000', '#Y_4data_15000', '#X_4data', '#X_2data_30000', '#X_1data_60000', '#X_8data']\n",
      "gridnode02's tags:  ['#Y_8data', '#X_2data_30000', '#X_2data', '#Y_2data', '#Y_4data', '#Y_4data_15000', '#Y_2data_30000', '#X_8data', '#X_4data', '#X_4data_15000']\n",
      "gridnode03's tags:  ['#X_4data', '#Y_4data', '#Y_8data', '#Y_4data_15000', '#X_8data', '#X_4data_15000']\n",
      "gridnode04's tags:  ['#Y_4data', '#X_8data', '#X_4data', '#X_4data_15000', '#Y_8data', '#Y_4data_15000']\n"
     ]
    }
   ],
   "source": [
    "for index in range(parties):\n",
    "\n",
    "    print(node_name[index]+\"'s tags: \", requests.get(address_list[index] + \"/data-centric/dataset-tags\").json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now go ahead and continue with  [2nd part](02-FL-mnist-train-model.ipynb) where we will train a Federated Deep Learning model from scratch without having data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PyGrid on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PyGrid](https://github.com/OpenMined/PyGrid)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PyGrid/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
