{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a remote Deep Learning model\n",
    "In this notebbok, we will show how to train a Federated Deep Learning with data hosted in Nodes.\n",
    "\n",
    "We will consider that you are a Data Scientist and you do not know where data lives, you only have access to GridNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Previous setup\n",
    "\n",
    "Components:\n",
    "\n",
    " - PyGrid Network      http://alice:7000\n",
    " - PyGrid Node Alice (http://bob:5000)\n",
    " - PyGrid Node Bob   (http://charlie:5001)\n",
    "\n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Here we import core dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syft and client configuration\n",
    "Now we hook Torch and connect to the GridNetwork. This is the only sever you do not need to know node addresses (networks knows), but lets first define some useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "parties = 8\n",
    "TAG_NAME = \"mnist_test_\"+str(parties)+\"nodes_ns\"\n",
    "#TAG_NAME = \"mnist_small\"\n",
    "\n",
    "#TAG_NAME = \"NPC_500_2nodes\"\n",
    "# mnist_test_8nodes  mnist_test_4nodes  mnist_test  mnist_test_small(4)   mnist_test_small2 (2node)\n",
    "grid_address = \"http://203.145.221.20:80\"  # address\n",
    "\n",
    "\n",
    "AGG_EPOCH = 2\n",
    "EPOCHS = 10\n",
    "N_EPOCHS = AGG_EPOCH*EPOCHS  # number of epochs to train\n",
    "N_TEST   = 128   # number of test\n",
    "train_batch_size = 16\n",
    "N_LOG = 1\n",
    "\n",
    "LR = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5\n",
    "\n",
    "node_name = [\"gridnode01\",\"gridnode02\",\"gridnode03\",\"gridnode04\",\"gridnode05\",\"gridnode06\",\"gridnode07\",\"gridnode08\"]\n",
    "output_model_folder = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "\n",
    "\n",
    "# Connect direcly to grid nodes\n",
    "my_grid = PublicGridNetwork(hook, grid_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Define our Neural Network Arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a Deep Learning Network, feel free to write your own model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.test_batch_size = N_TEST\n",
    "        self.epochs = N_EPOCHS\n",
    "        self.lr = LR\n",
    "        self.log_interval = N_LOG\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        #self.device = th.device(\"cpu\")\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "#device=[th.device(\"cuda:2\"),th.device(\"cuda:3\")]\n",
    "if(th.cuda.is_available()):\n",
    "     th.set_default_tensor_type(th.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small network (skip if use ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(5408, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.conv1(x)        \n",
    "        x = F.relu(x)        \n",
    "        #x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)        \n",
    "        #x = self.dropout1(x)\n",
    "        x = th.flatten(x, 1)        \n",
    "        x = self.fc1(x)        \n",
    "        x = F.relu(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Search for remote data\n",
    "\n",
    "Once we have defined our Deep Learning Network, we need some data to train... Thanks to PyGridNetwork this is very easy, you just need to search for your tags of interest.\n",
    "\n",
    "Notice that _search()_ method  returns a pointer tensor, so we will work with those keeping real tensors hosted in Alice and Bob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search fixed amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_grid.search(\"#X_\"+TAG_NAME)  # images\n",
    "target = my_grid.search(\"#Y_\"+TAG_NAME)  # labels\n",
    "\n",
    "data = list(data.values())  # returns a pointer\n",
    "target = list(target.values())  # returns a pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(Wrapper)>[PointerTensor | me:3448380049 -> gridnode01:97351606311]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...], [(Wrapper)>[PointerTensor | me:18147373791 -> gridnode03:55766307204]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...], [(Wrapper)>[PointerTensor | me:1064783392 -> gridnode04:65149853549]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...], [(Wrapper)>[PointerTensor | me:48059717666 -> gridnode05:78174810595]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...], [(Wrapper)>[PointerTensor | me:60555040428 -> gridnode06:49129793709]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...], [(Wrapper)>[PointerTensor | me:12419225111 -> gridnode07:95672945749]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...], [(Wrapper)>[PointerTensor | me:49804120409 -> gridnode08:60231340732]\n",
      "\tTags: #X_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500, 1, 28, 28])\n",
      "\tDescription: input mnist datapoinsts split 8 parties...]]\n",
      "[[(Wrapper)>[PointerTensor | me:35907514403 -> gridnode01:96566660548]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...], [(Wrapper)>[PointerTensor | me:48006828081 -> gridnode03:53663969122]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...], [(Wrapper)>[PointerTensor | me:12204279842 -> gridnode04:80924321511]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...], [(Wrapper)>[PointerTensor | me:49442118225 -> gridnode05:84712263382]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...], [(Wrapper)>[PointerTensor | me:2912991923 -> gridnode06:42826897987]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...], [(Wrapper)>[PointerTensor | me:22216426586 -> gridnode07:4376383848]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...], [(Wrapper)>[PointerTensor | me:89469648149 -> gridnode08:13567484526]\n",
      "\tTags: #Y_mnist_test_8nodes_ns \n",
      "\tShape: torch.Size([7500])\n",
      "\tDescription: input mnist labels split 8 parties...]]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import read_mnist_data\n",
    "transform = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,)),  #  mean and std \n",
    "                              ])\n",
    "#npz_path = '../'+str(parties)+'Parties/data_party0.npz'\n",
    "#trainloader,testloader = read_mnist_data(npz_path, batch = args.test_batch_size )\n",
    "testset = datasets.MNIST('../8node/dataset2', download=True, train=False, transform=transform)\n",
    "testloader = th.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original mnist train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('../8node/dataset2', download=False, train=True, transform=transform)\n",
    "trainloader = th.utils.data.DataLoader(trainset, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(parties):\n",
    "    model_list=[None] * parties\n",
    "    optims_list=[None] * parties\n",
    "    for i in range(parties):\n",
    "        model_list[i] = Net()\n",
    "#         model_list[i] = torchvision.models.resnet50(pretrained=False)\n",
    "        \n",
    "#         model_list[i].conv1 = th.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "#         batch = th.rand(4, 1, 224, 224) # in the NCHW format\n",
    "#         model_list[i](batch).size()\n",
    "        \n",
    "        \n",
    "        in_feature =  model_list[i].fc.in_features\n",
    "        model_list[i].fc = nn.Linear(in_feature, NUM_CLASSES)  # ch\n",
    "        model_list[i].to(device)\n",
    "        #optims_list[i] = Optims(workers, optim=optim.Adam(params=model_list[i].parameters(),lr=args.lr,weight_decay=args.weight_decay))\n",
    "        #optims_list[i] = Optims(workers, optim=optim.Adam(params=model_list[i].parameters(),lr=args.lr))\n",
    "        optims_list[i] = Optims(workers, optim=optim.SGD(params=model_list[i].parameters(),lr=args.lr, momentum = args.momentum,weight_decay=args.weight_decay))\n",
    "        #optims_list[i] = Optims(workers, optim=optim.SGD(params=model_list[i].parameters(),lr=args.lr, momentum = args.momentum))\n",
    "    return model_list, optims_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgWeight(model_list):\n",
    "    model_tmp=[None] * parties\n",
    "    optims_tmp=[None] * parties\n",
    "\n",
    "    for idx, my_model in enumerate(model_list):\n",
    "        \n",
    "        model_tmp[idx] = my_model.state_dict()\n",
    "\n",
    "\n",
    "    for key in model_tmp[0]:    \n",
    "        print(key)\n",
    "        model_sum = 0\n",
    "        for model_tmp_content in model_tmp:        \n",
    "            model_sum += model_tmp_content[key]\n",
    "            #print(model_tmp_content[key])\n",
    "        for i in range(len(model_tmp)):\n",
    "            #print(\"model_sum={}\".format(model_sum))\n",
    "            #print(\"len:{}\".format(len(model_tmp)))\n",
    "            model_avg = model_sum/len(model_tmp)\n",
    "            #print(\"model_avg={}\".format(model_avg))\n",
    "            model_tmp[i][key] = model_sum/len(model_tmp)\n",
    "    for i in range(len(model_list)):    \n",
    "        model_list[i].load_state_dict(model_tmp[i])\n",
    "        optims_tmp[i] = Optims(workers, optim=optim.SGD(params=model_list[i].parameters(),lr=args.lr, momentum = args.momentum,weight_decay=args.weight_decay))\n",
    "        #optims_tmp[i] = Optims(workers, optim=optim.Adam(params=model_list[i].parameters(),lr=args.lr))\n",
    "    return model_list, optims_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.federated.floptimizer import Optims\n",
    "\n",
    "workers =node_name[:parties]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_list, optims_list = init_model(parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(curr_model, curr_optims, args):\n",
    "#    shuffle_list = np.arange(len(datalist))\n",
    "#    np.random.shuffle(shuffle_list)\n",
    "#    datalist = datalist[shuffle_list]\n",
    "#    targetlist = targetlist[shuffle_list]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        curr_model[i].train()\n",
    "        print(next(curr_model[i].parameters()).is_cuda )\n",
    "        \n",
    "        # This loop is for \"a bunch of data\" searched on the node.\n",
    "        # Equals to an epoch for a node if there is only \"one bunch of data\" for a node. \n",
    "        loss_epoch = 0\n",
    "        for j in range(len(data[i])):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            worker = data[i][j].location  # worker hosts data\n",
    "            print(worker.id)\n",
    "            if worker.id not in workers:\n",
    "                print(\"not in worker list\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            data_device = data[i][j].to(device)\n",
    "            target_device = target[i][j].to(device)\n",
    "\n",
    "            curr_model[i].send(worker)  # send model to PyGridNode worker\n",
    "\n",
    "            batch_remainder = len(data[i][j])%train_batch_size\n",
    "\n",
    "\n",
    "            for k in range(len(data[i][j])//train_batch_size):\n",
    "                optimizer = curr_optims[i].get_optim(worker.id)   \n",
    "\n",
    "                optimizer.zero_grad()  \n",
    "                pred = curr_model[i](data_device[k*train_batch_size:(k+1)*train_batch_size])\n",
    "                loss = criterion(pred, target_device[k*train_batch_size:(k+1)*train_batch_size])\n",
    "                #loss = F.nll_loss(pred, target[i][j])\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.get().item()\n",
    "                \n",
    "            k+=1\n",
    "\n",
    "            if batch_remainder != 0:\n",
    "                optimizer = curr_optims[i].get_optim(worker.id)   \n",
    "                optimizer.zero_grad()  \n",
    "                pred = curr_model[i](data_device[k*train_batch_size:k*train_batch_size+batch_remainder])\n",
    "                loss = criterion(pred, target_device[k*train_batch_size:k*train_batch_size+batch_remainder])\n",
    "                #loss = F.nll_loss(pred, target[i][j])\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.get().item()\n",
    "\n",
    "            curr_model[i].get()  # get back the model\n",
    "            print(next(curr_model[i].parameters()).is_cuda )\n",
    "\n",
    "\n",
    "        th.save(curr_model[i].state_dict(), f'{output_model_folder}/checkpoint_{epoch}_{i}.pth')    \n",
    "\n",
    "        if epoch % args.log_interval == 0:\n",
    "\n",
    "            print('Train Epoch: {} | With {} data |: \\tLoss: {:.6f}'.format(\n",
    "                      epoch, worker.id,  loss_epoch))\n",
    "\n",
    "    return curr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_model, args,fo):\n",
    "    \n",
    "    if epoch % args.log_interval == 0:\n",
    "    \n",
    "        test_model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in testloader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = test_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss  #F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        fo.write(\"{},{:.4f},{:.2f}\\n\".format(epoch, test_loss,100. * correct / len(testloader.dataset)))   \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testloader.dataset),\n",
    "            100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_acc(test_model, args):\n",
    "    \n",
    "    if epoch % args.log_interval == 0:\n",
    "    \n",
    "        test_model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in trainloader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = test_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss  #F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(trainloader.dataset)\n",
    "\n",
    "        print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(trainloader.dataset),\n",
    "            100. * correct / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "gridnode01\n",
      "True\n",
      "Train Epoch: 0 | With gridnode01 data |: \tLoss: 3605.654568\n",
      "True\n",
      "gridnode03\n",
      "True\n",
      "Train Epoch: 0 | With gridnode03 data |: \tLoss: 3507.154406\n",
      "True\n",
      "gridnode04\n",
      "True\n",
      "Train Epoch: 0 | With gridnode04 data |: \tLoss: 3198.300802\n",
      "True\n",
      "gridnode05\n",
      "True\n",
      "Train Epoch: 0 | With gridnode05 data |: \tLoss: 3658.790111\n",
      "True\n",
      "gridnode06\n",
      "True\n",
      "Train Epoch: 0 | With gridnode06 data |: \tLoss: 4034.247991\n",
      "True\n",
      "gridnode07\n",
      "True\n",
      "Train Epoch: 0 | With gridnode07 data |: \tLoss: 3253.042799\n",
      "True\n",
      "gridnode08\n",
      "True\n",
      "Train Epoch: 0 | With gridnode08 data |: \tLoss: 3744.923020\n",
      "----before aggregation----\n",
      "\n",
      "Test set: Average loss: 121.5383, Accuracy: 979/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 22.2885, Accuracy: 1024/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0312, Accuracy: 1319/10000 (13%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 18.8499, Accuracy: 1023/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7293, Accuracy: 1052/10000 (11%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.1350, Accuracy: 1410/10000 (14%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 86.9587, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0202, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "True\n",
      "gridnode01\n",
      "True\n",
      "Train Epoch: 1 | With gridnode01 data |: \tLoss: 1695.674117\n",
      "True\n",
      "gridnode03\n"
     ]
    }
   ],
   "source": [
    "#AGG_EPOCH = 2\n",
    "\n",
    "output_file_name = TAG_NAME+'_AGG'+str(AGG_EPOCH)+'.csv'\n",
    "fo = open(output_file_name, \"w\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    \n",
    "    current_model = train(model_list,optims_list, args)\n",
    "    \n",
    "    print(\"----before aggregation----\")\n",
    "    for test_model in current_model:\n",
    "        test(test_model, args,fo)\n",
    "        \n",
    "     \n",
    "    if (epoch+1) % AGG_EPOCH ==0:\n",
    "        ## model avg\n",
    "        model_list,optims_list = avgWeight(current_model)\n",
    "        print(\"----after aggregation----\")\n",
    "        for test_model in model_list:\n",
    "            test(test_model, args,fo)\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight True\n",
      "conv1.weight None\n",
      "conv1.bias True\n",
      "conv1.bias None\n",
      "fc1.weight True\n",
      "fc1.weight None\n",
      "fc1.bias True\n",
      "fc1.bias None\n",
      "fc2.weight True\n",
      "fc2.weight None\n",
      "fc2.bias True\n",
      "fc2.bias None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "NUM_CLASSES = 10\n",
    "model = Net()\n",
    "in_feature =  model.fc.in_features\n",
    "model.fc = nn.Linear(in_feature, NUM_CLASSES)  # ch\n",
    "model.to(device)\n",
    "#with torch.no_grad():\n",
    "for name, param in model.named_parameters():\n",
    "    print(\"{} {}\".format(name, param.requires_grad))\n",
    "\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.sum())\n",
    "    else:\n",
    "        print(name, param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[-0.1328, -0.1019,  0.0785],\n",
       "                        [ 0.2806, -0.3180, -0.2036],\n",
       "                        [-0.2728,  0.1638,  0.2065]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2845,  0.1327, -0.0324],\n",
       "                        [-0.2365, -0.2362, -0.3152],\n",
       "                        [ 0.0370,  0.0167, -0.3030]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3574, -0.2948, -0.0616],\n",
       "                        [ 0.1574,  0.2851,  0.2707],\n",
       "                        [ 0.0625,  0.1973, -0.1525]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2383,  0.1192, -0.0877],\n",
       "                        [-0.2530,  0.1197, -0.0134],\n",
       "                        [-0.1103,  0.0722,  0.2642]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2541,  0.3477, -0.2550],\n",
       "                        [-0.2919, -0.2888, -0.1888],\n",
       "                        [-0.1940,  0.0830,  0.3208]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1380,  0.0663, -0.0064],\n",
       "                        [-0.1688, -0.1088, -0.0675],\n",
       "                        [ 0.2484, -0.0724,  0.0926]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0870,  0.0022,  0.2481],\n",
       "                        [ 0.2315, -0.3020,  0.1758],\n",
       "                        [ 0.2698,  0.2693, -0.1173]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2990,  0.0560,  0.2254],\n",
       "                        [ 0.0570, -0.3001,  0.0856],\n",
       "                        [ 0.1874,  0.2220, -0.1067]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3278,  0.0540,  0.1904],\n",
       "                        [ 0.1343,  0.1552, -0.3146],\n",
       "                        [-0.0926, -0.3780,  0.1293]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0894, -0.2136, -0.2552],\n",
       "                        [ 0.0831,  0.1093, -0.2833],\n",
       "                        [ 0.2332,  0.2541,  0.1453]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2576,  0.1422,  0.1963],\n",
       "                        [-0.2993,  0.1457,  0.1684],\n",
       "                        [-0.0571,  0.0269, -0.0123]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1859, -0.2450,  0.0590],\n",
       "                        [-0.1872, -0.2717,  0.0175],\n",
       "                        [-0.0629,  0.1944, -0.2166]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0559, -0.2111,  0.1333],\n",
       "                        [-0.0442,  0.0326, -0.2289],\n",
       "                        [-0.3872,  0.1299,  0.2571]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0058, -0.3304, -0.0772],\n",
       "                        [ 0.0593, -0.1494,  0.2678],\n",
       "                        [ 0.0098,  0.1558,  0.2715]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1911,  0.1413, -0.0743],\n",
       "                        [ 0.2145,  0.0887, -0.2648],\n",
       "                        [-0.3388, -0.2287, -0.3695]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2943, -0.2604,  0.3195],\n",
       "                        [-0.0609, -0.1064, -0.1642],\n",
       "                        [-0.2612,  0.3608,  0.0355]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0122, -0.2464, -0.0119],\n",
       "                        [-0.0865, -0.0856, -0.0005],\n",
       "                        [ 0.1367,  0.0766, -0.2518]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2065, -0.1060, -0.1426],\n",
       "                        [-0.0559, -0.0752, -0.1170],\n",
       "                        [ 0.3120,  0.0708, -0.2961]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0605,  0.2688, -0.2556],\n",
       "                        [-0.1088, -0.1370,  0.0797],\n",
       "                        [-0.3331, -0.2449,  0.0997]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3047,  0.0182,  0.1491],\n",
       "                        [ 0.1001, -0.2672,  0.1612],\n",
       "                        [ 0.0151,  0.0972, -0.3138]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0716, -0.3554,  0.0989],\n",
       "                        [-0.0068,  0.2774, -0.1824],\n",
       "                        [ 0.0713,  0.1440, -0.2510]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1958,  0.0613, -0.1845],\n",
       "                        [ 0.2361, -0.2108,  0.0794],\n",
       "                        [-0.1426,  0.1491, -0.2041]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2137, -0.0241, -0.0742],\n",
       "                        [ 0.0523, -0.1242,  0.3243],\n",
       "                        [ 0.1448, -0.1869,  0.0282]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1587,  0.2462, -0.2891],\n",
       "                        [-0.3653,  0.1754, -0.3791],\n",
       "                        [ 0.1317, -0.0331,  0.2396]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1507,  0.2144,  0.2139],\n",
       "                        [-0.0054, -0.2955, -0.1002],\n",
       "                        [ 0.2906, -0.1920, -0.2153]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0328,  0.0557,  0.0204],\n",
       "                        [ 0.0256, -0.2792,  0.0868],\n",
       "                        [-0.2705, -0.1830, -0.2744]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3350,  0.2535,  0.0506],\n",
       "                        [ 0.1786,  0.0865, -0.0484],\n",
       "                        [ 0.1173,  0.1804, -0.3305]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0797, -0.1468,  0.2558],\n",
       "                        [ 0.1215,  0.0981,  0.1515],\n",
       "                        [-0.2069, -0.0416,  0.1799]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2478, -0.2734, -0.1590],\n",
       "                        [ 0.1760, -0.3338, -0.0464],\n",
       "                        [-0.2831,  0.0415, -0.3050]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0279, -0.0222, -0.0877],\n",
       "                        [-0.3400, -0.3698, -0.2610],\n",
       "                        [-0.0112,  0.0825,  0.0581]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1409, -0.1064,  0.0936],\n",
       "                        [ 0.0959,  0.2751, -0.1702],\n",
       "                        [ 0.0462, -0.0968, -0.1444]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1595, -0.2874, -0.0665],\n",
       "                        [-0.2537,  0.0773,  0.1008],\n",
       "                        [-0.3563,  0.2728,  0.0715]]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.2748,  0.1612,  0.0304,  0.0905, -0.2098,  0.2237, -0.3577, -0.0399,\n",
       "                       0.0674, -0.0614,  0.0538,  0.1296,  0.0258, -0.1381, -0.1831, -0.1867,\n",
       "                       0.0621, -0.1666, -0.0952,  0.1160,  0.0346, -0.2339, -0.1953,  0.0556,\n",
       "                      -0.3081, -0.2102,  0.0468, -0.1499, -0.0856,  0.1092,  0.0909,  0.1587],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0117,  0.0069,  0.0074,  ..., -0.0367, -0.0248, -0.0223],\n",
       "                      [ 0.0011, -0.0109, -0.0089,  ..., -0.0314, -0.0440, -0.0476],\n",
       "                      [ 0.0058,  0.0107,  0.0055,  ..., -0.0298, -0.0360, -0.0248],\n",
       "                      ...,\n",
       "                      [-0.0121,  0.0031,  0.0067,  ...,  0.0049,  0.0002, -0.0021],\n",
       "                      [-0.0080, -0.0085, -0.0064,  ...,  0.0165,  0.0225,  0.0277],\n",
       "                      [-0.0130,  0.0056, -0.0075,  ..., -0.0367, -0.0377, -0.0363]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0339, -0.0306, -0.0324, -0.0353,  0.0287, -0.0040, -0.0273,  0.0242,\n",
       "                      -0.0277, -0.0196, -0.0476, -0.0208, -0.0175, -0.0092, -0.0075, -0.0363,\n",
       "                      -0.0341, -0.0437,  0.0008, -0.0254, -0.0047, -0.0410, -0.0291, -0.0142,\n",
       "                      -0.0324, -0.0065, -0.0235, -0.0278,  0.0106, -0.0437, -0.0148, -0.0258,\n",
       "                      -0.0351, -0.0416, -0.0294, -0.0364, -0.0070, -0.0310,  0.0032, -0.0382,\n",
       "                      -0.0285, -0.0008,  0.0163, -0.0132, -0.0103, -0.0248, -0.0384,  0.0118,\n",
       "                      -0.0243, -0.0068, -0.0415, -0.0051,  0.0066, -0.0296,  0.0059, -0.0286,\n",
       "                      -0.0089, -0.0359, -0.0336, -0.0469, -0.0411,  0.0037,  0.0032, -0.0375,\n",
       "                      -0.0273,  0.0305, -0.0284, -0.0387, -0.0429, -0.0008, -0.0262,  0.0375,\n",
       "                      -0.0033, -0.0386, -0.0326,  0.0019, -0.0353, -0.0364, -0.0237,  0.0274,\n",
       "                      -0.0379, -0.0354, -0.0352, -0.0350, -0.0333, -0.0161, -0.0475, -0.0233,\n",
       "                      -0.0229, -0.0236, -0.0278,  0.0159, -0.0360, -0.0341, -0.0277, -0.0128,\n",
       "                      -0.0329, -0.0246, -0.0200,  0.0119,  0.0219, -0.0262, -0.0284, -0.0467,\n",
       "                      -0.0370, -0.0282, -0.0266, -0.0198, -0.0251, -0.0062, -0.0404, -0.0029,\n",
       "                      -0.0474, -0.0320, -0.0471, -0.0073, -0.0081, -0.0448, -0.0300, -0.0225,\n",
       "                      -0.0249, -0.0408, -0.0242, -0.0363, -0.0381, -0.0321, -0.0395,  0.0165,\n",
       "                      -0.0236, -0.0271, -0.0029, -0.0201,  0.0023,  0.0268, -0.0346, -0.0153,\n",
       "                      -0.0135, -0.0065, -0.0153, -0.0264, -0.0331, -0.0443,  0.0201, -0.0166,\n",
       "                      -0.0411, -0.0385, -0.0207, -0.0205, -0.0368, -0.0273, -0.0238, -0.0244,\n",
       "                      -0.0323, -0.0394,  0.0071, -0.0417, -0.0459, -0.0363, -0.0033, -0.0289,\n",
       "                      -0.0414, -0.0357, -0.0193, -0.0394, -0.0198,  0.0067, -0.0364, -0.0367,\n",
       "                       0.0003, -0.0322, -0.0347, -0.0213, -0.0061, -0.0210, -0.0233,  0.0039,\n",
       "                      -0.0294,  0.0119, -0.0294, -0.0229, -0.0332, -0.0181, -0.0136, -0.0261,\n",
       "                      -0.0271, -0.0322, -0.0212, -0.0241, -0.0390, -0.0316, -0.0264, -0.0397,\n",
       "                       0.0224, -0.0435, -0.0281, -0.0107, -0.0309, -0.0389, -0.0041, -0.0199,\n",
       "                      -0.0475, -0.0195, -0.0421,  0.0040, -0.0008, -0.0048, -0.0113,  0.0037,\n",
       "                      -0.0109, -0.0281, -0.0096,  0.0114,  0.0216,  0.0051, -0.0292, -0.0470,\n",
       "                       0.0130,  0.0059, -0.0245, -0.0269, -0.0265, -0.0287, -0.0378, -0.0138,\n",
       "                      -0.0176, -0.0421, -0.0393, -0.0232, -0.0417, -0.0251,  0.0009,  0.0096,\n",
       "                      -0.0488, -0.0383, -0.0231, -0.0321, -0.0095, -0.0231,  0.0013, -0.0336,\n",
       "                      -0.0403, -0.0398, -0.0287, -0.0261, -0.0394, -0.0332, -0.0076, -0.0011,\n",
       "                      -0.0109,  0.0035, -0.0288,  0.0186, -0.0268,  0.0158,  0.0232, -0.0328],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0263,  0.0192,  0.0172,  ...,  0.0436, -0.0245,  0.0030],\n",
       "                      [-0.0130, -0.0420, -0.0926,  ..., -0.0472,  0.0145,  0.0261],\n",
       "                      [-0.0428,  0.0961, -0.0847,  ...,  0.0499,  0.0456, -0.0237],\n",
       "                      ...,\n",
       "                      [-0.0101, -0.0486, -0.0583,  ..., -0.0206, -0.0718, -0.0442],\n",
       "                      [-0.0334, -0.0186,  0.0172,  ..., -0.0582, -0.0340,  0.0046],\n",
       "                      [ 0.0823, -0.0258, -0.0225,  ...,  0.0013,  0.0336, -0.0815]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 0.0638,  0.0132, -0.0016,  0.0344,  0.0492, -0.0203,  0.0548,  0.0694,\n",
       "                      -0.0624,  0.0343], device='cuda:0'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list[3].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[-3.1955e-02, -1.5596e-01,  1.3126e-01],\n",
       "                        [ 1.1209e-01, -2.4137e-02, -1.2187e-01],\n",
       "                        [-9.8066e-02, -1.5323e-02, -5.1719e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2562e-02, -6.3484e-02, -1.0979e-01],\n",
       "                        [-1.9259e-01, -1.4969e-01, -2.9122e-02],\n",
       "                        [ 1.0481e-01, -6.9001e-02, -6.4005e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0426e-01, -5.5101e-02, -6.6211e-02],\n",
       "                        [ 8.7659e-02,  1.8548e-01,  7.9895e-02],\n",
       "                        [-1.6249e-01,  6.4304e-02, -1.0850e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.6541e-03, -5.4680e-02, -7.4961e-02],\n",
       "                        [-1.2949e-01, -7.5531e-02,  1.5874e-02],\n",
       "                        [ 3.1721e-03,  1.3787e-01,  1.6286e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4306e-01,  1.7598e-02, -9.9530e-03],\n",
       "                        [-5.7359e-02, -2.9488e-01,  2.8952e-02],\n",
       "                        [-5.7886e-03, -9.4434e-02,  8.0928e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5943e-02, -1.3977e-01, -1.2968e-01],\n",
       "                        [-4.9301e-02, -1.8151e-01, -4.6370e-02],\n",
       "                        [-1.6308e-01, -1.5081e-01, -5.4957e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.4229e-02, -8.6571e-02,  2.0085e-01],\n",
       "                        [ 1.9918e-01, -2.0178e-01, -1.0617e-01],\n",
       "                        [ 7.2476e-04,  1.5915e-01, -1.3515e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9049e-01, -5.3776e-03,  4.9801e-02],\n",
       "                        [ 7.2228e-03, -1.6190e-01,  2.6448e-02],\n",
       "                        [-8.9228e-02, -7.6042e-02, -1.8260e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1606e-01, -2.2336e-01,  6.3039e-02],\n",
       "                        [-1.8579e-02,  3.5604e-02, -2.0953e-01],\n",
       "                        [ 7.8623e-02, -4.3934e-02, -1.4983e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4937e-01, -1.6954e-01, -1.5593e-01],\n",
       "                        [-6.4589e-03, -1.5288e-01, -1.7639e-01],\n",
       "                        [-1.0402e-01,  2.7393e-02,  6.1292e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9480e-01, -3.8141e-02, -6.3633e-02],\n",
       "                        [-2.7656e-01,  3.7682e-02,  4.5966e-03],\n",
       "                        [-1.1264e-01, -4.0234e-02, -2.0591e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8679e-02, -1.6782e-01, -1.0493e-01],\n",
       "                        [ 6.8287e-02, -2.8918e-01,  1.8874e-01],\n",
       "                        [ 4.6104e-02, -1.1700e-01, -3.9223e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2801e-01, -2.4530e-01, -1.3490e-01],\n",
       "                        [ 7.1692e-02,  1.1438e-01, -7.8431e-02],\n",
       "                        [-1.6199e-01,  4.0369e-03,  1.7531e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5301e-01, -2.2596e-01,  1.3188e-02],\n",
       "                        [-1.5059e-01,  5.4642e-02,  1.2413e-01],\n",
       "                        [-4.4809e-02,  2.4296e-02,  8.2675e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2284e-02, -9.1968e-03, -1.1731e-01],\n",
       "                        [ 2.2295e-02, -9.8442e-02, -2.3625e-02],\n",
       "                        [-1.3404e-01,  1.6614e-02, -1.2113e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3299e-02,  6.1744e-02, -5.6682e-02],\n",
       "                        [-9.9396e-02, -1.0969e-02, -1.7642e-01],\n",
       "                        [-5.3815e-02,  2.2541e-01,  9.3062e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1314e-01, -1.8390e-01, -3.9548e-02],\n",
       "                        [ 2.2335e-02,  1.8370e-02,  4.9171e-02],\n",
       "                        [-8.2754e-02, -2.0745e-02, -1.1067e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4376e-01, -7.9147e-02, -1.8335e-01],\n",
       "                        [-3.3330e-03, -5.1430e-02,  5.6430e-02],\n",
       "                        [ 8.3015e-02, -1.6618e-01, -6.6084e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.6211e-03,  5.7346e-02, -1.7356e-01],\n",
       "                        [-1.7176e-01, -1.5412e-01, -2.6155e-02],\n",
       "                        [-1.1167e-01,  2.3848e-02, -1.5674e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7300e-01, -2.6950e-02, -3.9270e-02],\n",
       "                        [ 9.2712e-02, -9.3208e-02, -7.3130e-02],\n",
       "                        [-3.0888e-03,  4.7933e-02, -1.7327e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.8022e-02, -1.6565e-01,  1.0772e-01],\n",
       "                        [-4.4786e-02,  2.0618e-01,  1.4162e-02],\n",
       "                        [-9.7802e-02, -8.9201e-02, -7.1498e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6020e-02,  2.0798e-01, -3.2764e-02],\n",
       "                        [ 2.7455e-02,  7.7184e-02,  7.0450e-02],\n",
       "                        [-1.0631e-01, -2.7927e-02, -1.5112e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.1542e-02, -3.3114e-02, -8.9101e-03],\n",
       "                        [-3.9824e-03, -6.6298e-02, -7.3489e-02],\n",
       "                        [-9.4704e-02, -8.3996e-02,  1.6582e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0362e-02,  1.2843e-01, -8.7174e-02],\n",
       "                        [-3.6457e-02, -7.0832e-02, -3.5562e-03],\n",
       "                        [-2.2797e-02, -4.4129e-02,  1.0594e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1458e-01,  2.3399e-03, -7.4903e-02],\n",
       "                        [ 9.4056e-02, -2.1119e-01, -1.1110e-01],\n",
       "                        [ 1.7019e-01, -1.0115e-01, -4.2402e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1411e-01, -5.3194e-04, -1.8577e-01],\n",
       "                        [ 2.8411e-02, -5.9543e-02, -5.5154e-02],\n",
       "                        [-2.5281e-02,  1.0241e-01,  2.5740e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5949e-01,  1.1814e-03,  6.9844e-02],\n",
       "                        [-1.3152e-02, -1.2910e-01, -7.9456e-03],\n",
       "                        [-1.6393e-01,  3.2748e-03, -2.3032e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.5744e-02, -4.7232e-02, -1.3597e-02],\n",
       "                        [-1.7027e-01,  1.8920e-04,  6.6017e-02],\n",
       "                        [-1.2991e-01, -1.5603e-02,  8.9173e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.0291e-02, -2.3421e-01, -7.9086e-02],\n",
       "                        [-6.5003e-04, -2.0186e-01, -3.9719e-02],\n",
       "                        [ 1.6216e-02,  3.2607e-02, -2.9217e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7729e-02, -1.3446e-01, -1.0013e-01],\n",
       "                        [-3.7095e-02, -9.7603e-02, -1.7789e-01],\n",
       "                        [-1.3865e-03, -4.8321e-02, -1.7162e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3145e-01,  9.2846e-03, -9.6798e-02],\n",
       "                        [-9.3967e-02, -1.1653e-01, -2.5649e-01],\n",
       "                        [-1.6813e-01,  5.7612e-03, -7.8137e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2587e-02, -1.5424e-01, -5.7319e-02],\n",
       "                        [-1.8839e-01,  9.9558e-02,  1.4958e-01],\n",
       "                        [-7.6741e-02,  1.3090e-02, -1.0501e-01]]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.1435,  0.0542, -0.1351, -0.0988, -0.1003,  0.1164, -0.2919,  0.1078,\n",
       "                      -0.2064, -0.0709,  0.1013,  0.0206,  0.0022, -0.0701, -0.1001, -0.1749,\n",
       "                      -0.1125,  0.0233, -0.0732,  0.0324,  0.0241, -0.2411, -0.0752, -0.0037,\n",
       "                      -0.1288, -0.1023, -0.0667, -0.0398, -0.0040, -0.0850, -0.1566, -0.1246],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0015,  0.0086,  0.0034,  ..., -0.0240, -0.0243, -0.0143],\n",
       "                      [-0.0031, -0.0055, -0.0035,  ..., -0.0223, -0.0247, -0.0311],\n",
       "                      [-0.0009,  0.0041, -0.0006,  ..., -0.0272, -0.0258, -0.0209],\n",
       "                      ...,\n",
       "                      [-0.0064, -0.0022,  0.0023,  ..., -0.0053, -0.0045, -0.0141],\n",
       "                      [ 0.0017,  0.0015,  0.0040,  ..., -0.0085, -0.0072, -0.0080],\n",
       "                      [-0.0048,  0.0007, -0.0010,  ..., -0.0161, -0.0129, -0.0202]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0444, -0.0290, -0.0506, -0.0391, -0.0352, -0.0283, -0.0418, -0.0081,\n",
       "                      -0.0356, -0.0366, -0.0432, -0.0328, -0.0215, -0.0077, -0.0185, -0.0428,\n",
       "                      -0.0429, -0.0527, -0.0211, -0.0475, -0.0208,  0.0058, -0.0536, -0.0178,\n",
       "                      -0.0257, -0.0191, -0.0334, -0.0414, -0.0385, -0.0362, -0.0406, -0.0239,\n",
       "                      -0.0402, -0.0459, -0.0300, -0.0527, -0.0320, -0.0225, -0.0215, -0.0342,\n",
       "                      -0.0463, -0.0336, -0.0264, -0.0370, -0.0198, -0.0493, -0.0185, -0.0310,\n",
       "                      -0.0279, -0.0329, -0.0485, -0.0267, -0.0150, -0.0220, -0.0317, -0.0348,\n",
       "                      -0.0264, -0.0404, -0.0372, -0.0441, -0.0246, -0.0179, -0.0261, -0.0360,\n",
       "                      -0.0345, -0.0189, -0.0302, -0.0285, -0.0554, -0.0351, -0.0299, -0.0064,\n",
       "                      -0.0394, -0.0312, -0.0484, -0.0465, -0.0366, -0.0363, -0.0341, -0.0224,\n",
       "                      -0.0413, -0.0282, -0.0458, -0.0320, -0.0303, -0.0380, -0.0337, -0.0390,\n",
       "                      -0.0332, -0.0429, -0.0320, -0.0083, -0.0245, -0.0257, -0.0147, -0.0406,\n",
       "                      -0.0422, -0.0399, -0.0381, -0.0364, -0.0306, -0.0329, -0.0408, -0.0477,\n",
       "                      -0.0426, -0.0160, -0.0242, -0.0391, -0.0496, -0.0231, -0.0348, -0.0257,\n",
       "                      -0.0249, -0.0411, -0.0290, -0.0400, -0.0194, -0.0363, -0.0185, -0.0410,\n",
       "                      -0.0271, -0.0474, -0.0200, -0.0311, -0.0461, -0.0415, -0.0346, -0.0299,\n",
       "                      -0.0476, -0.0345, -0.0261, -0.0247, -0.0461, -0.0311, -0.0214, -0.0354,\n",
       "                      -0.0189, -0.0312, -0.0319, -0.0290, -0.0400, -0.0352, -0.0236, -0.0225,\n",
       "                      -0.0554, -0.0495, -0.0300, -0.0295, -0.0371, -0.0512, -0.0416, -0.0212,\n",
       "                      -0.0495, -0.0142,  0.0081, -0.0275, -0.0344, -0.0409, -0.0215, -0.0407,\n",
       "                      -0.0501, -0.0335, -0.0343, -0.0304, -0.0336, -0.0295, -0.0430, -0.0450,\n",
       "                      -0.0358, -0.0281, -0.0437, -0.0361, -0.0323, -0.0360, -0.0385, -0.0278,\n",
       "                      -0.0327, -0.0407, -0.0451, -0.0385, -0.0515, -0.0352, -0.0409, -0.0492,\n",
       "                      -0.0385, -0.0475, -0.0257, -0.0393, -0.0543, -0.0388, -0.0236, -0.0424,\n",
       "                       0.0118, -0.0441, -0.0294, -0.0384, -0.0261, -0.0326, -0.0396, -0.0237,\n",
       "                      -0.0519, -0.0396, -0.0515, -0.0333, -0.0310, -0.0189, -0.0205, -0.0217,\n",
       "                      -0.0424, -0.0263, -0.0336, -0.0199, -0.0232, -0.0228, -0.0293, -0.0444,\n",
       "                      -0.0279, -0.0195, -0.0189, -0.0281, -0.0370, -0.0479, -0.0357, -0.0298,\n",
       "                      -0.0315, -0.0371, -0.0495, -0.0436, -0.0481, -0.0239, -0.0051, -0.0340,\n",
       "                      -0.0362, -0.0399, -0.0145, -0.0331, -0.0320, -0.0447, -0.0097, -0.0281,\n",
       "                      -0.0420, -0.0421, -0.0224, -0.0447, -0.0259, -0.0228, -0.0378, -0.0378,\n",
       "                      -0.0459, -0.0221, -0.0393, -0.0377, -0.0388, -0.0231, -0.0209, -0.0356],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0140,  0.0116,  0.0032,  ..., -0.0005,  0.0446,  0.0136],\n",
       "                      [-0.0191, -0.0023, -0.0165,  ...,  0.0045, -0.0115,  0.0193],\n",
       "                      [-0.0268,  0.0129, -0.0466,  ...,  0.0343,  0.0201,  0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0123,  0.0068, -0.0104,  ...,  0.0162, -0.0414, -0.0058],\n",
       "                      [-0.0339,  0.0023,  0.0094,  ..., -0.0023, -0.0036, -0.0007],\n",
       "                      [-0.0246,  0.0003,  0.0573,  ...,  0.0436,  0.0417, -0.0322]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 0.0046, -0.0112,  0.0318,  0.0054,  0.0341, -0.0038,  0.0173,  0.0377,\n",
       "                      -0.0114,  0.0119], device='cuda:0'))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list[1].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Here you are, you have trained a model on remote data using Federated Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=tensor([[[[-0.1136,  0.0119,  0.0643],\n",
      "          [ 0.2360, -0.2665,  0.3035],\n",
      "          [-0.1654,  0.0671,  0.0101]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2986,  0.0780, -0.2830],\n",
      "          [ 0.1987, -0.0251,  0.3257],\n",
      "          [-0.3063,  0.3294,  0.2357]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1051,  0.1047, -0.2119],\n",
      "          [-0.0992,  0.0779,  0.0424],\n",
      "          [ 0.1413, -0.1650,  0.0465]]],\n",
      "\n",
      "\n",
      "        [[[-0.2344, -0.1020,  0.0777],\n",
      "          [ 0.1065,  0.0464, -0.2997],\n",
      "          [ 0.0662,  0.2825, -0.0874]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0143, -0.1704,  0.0094],\n",
      "          [-0.2515, -0.2792, -0.2568],\n",
      "          [-0.0121,  0.3080, -0.1566]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0484,  0.2494, -0.0696],\n",
      "          [-0.2811, -0.0548, -0.0820],\n",
      "          [-0.1393, -0.0216, -0.3121]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1564, -0.0025,  0.0601],\n",
      "          [-0.1601, -0.2392, -0.0638],\n",
      "          [-0.2685, -0.3021,  0.1344]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0864, -0.2734,  0.1815],\n",
      "          [ 0.2332,  0.1287,  0.1620],\n",
      "          [-0.3181,  0.3021,  0.3328]]],\n",
      "\n",
      "\n",
      "        [[[-0.0173,  0.1830,  0.1662],\n",
      "          [ 0.1283,  0.1086, -0.1821],\n",
      "          [ 0.1350, -0.3280, -0.0983]]],\n",
      "\n",
      "\n",
      "        [[[-0.1681, -0.1739,  0.2942],\n",
      "          [-0.2608,  0.1991,  0.3245],\n",
      "          [-0.0698, -0.1418, -0.2661]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1064, -0.3006,  0.3173],\n",
      "          [-0.3147,  0.1160,  0.0250],\n",
      "          [-0.3159, -0.0922, -0.3074]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0621,  0.2817,  0.0285],\n",
      "          [-0.2068, -0.2526, -0.1243],\n",
      "          [-0.0018,  0.0973, -0.0768]]],\n",
      "\n",
      "\n",
      "        [[[-0.0399,  0.1295, -0.1041],\n",
      "          [-0.0735, -0.2167,  0.1636],\n",
      "          [-0.0225,  0.3236,  0.2001]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1163, -0.2519,  0.0061],\n",
      "          [-0.1071,  0.0130, -0.1643],\n",
      "          [ 0.0326,  0.3150, -0.2923]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0793,  0.2944, -0.1597],\n",
      "          [-0.2749, -0.0458, -0.2455],\n",
      "          [-0.1038,  0.3209,  0.0571]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1837,  0.0551, -0.0864],\n",
      "          [ 0.0711, -0.0599,  0.1751],\n",
      "          [ 0.0341, -0.2366,  0.1005]]],\n",
      "\n",
      "\n",
      "        [[[-0.2372, -0.0315, -0.2571],\n",
      "          [ 0.0554,  0.2141, -0.0213],\n",
      "          [-0.0440,  0.0841, -0.0452]]],\n",
      "\n",
      "\n",
      "        [[[-0.1328,  0.3019, -0.1000],\n",
      "          [ 0.0895,  0.2334,  0.0756],\n",
      "          [ 0.2999,  0.0270,  0.1651]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1005, -0.2371,  0.0741],\n",
      "          [ 0.0648,  0.2013,  0.0784],\n",
      "          [ 0.2533, -0.1113,  0.1293]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1724,  0.3322, -0.1641],\n",
      "          [-0.2607,  0.0381,  0.0856],\n",
      "          [-0.2861, -0.0544,  0.0318]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0722,  0.0272, -0.2007],\n",
      "          [ 0.0187, -0.2056, -0.1721],\n",
      "          [ 0.0930,  0.1580, -0.0293]]],\n",
      "\n",
      "\n",
      "        [[[-0.2784,  0.2991, -0.1696],\n",
      "          [-0.2040, -0.0115,  0.2155],\n",
      "          [ 0.0462, -0.1645, -0.0773]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1869,  0.3041,  0.2982],\n",
      "          [-0.0735, -0.2434,  0.1276],\n",
      "          [ 0.1824, -0.2272, -0.1989]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1348, -0.1582, -0.0890],\n",
      "          [ 0.0192, -0.0988,  0.2214],\n",
      "          [-0.2626, -0.3320,  0.2457]]],\n",
      "\n",
      "\n",
      "        [[[-0.1095,  0.1116, -0.2196],\n",
      "          [-0.2562, -0.0876, -0.1637],\n",
      "          [ 0.1215, -0.1174, -0.1389]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0543,  0.2037, -0.0015],\n",
      "          [-0.2545, -0.0224, -0.1717],\n",
      "          [-0.3035, -0.2216, -0.3151]]],\n",
      "\n",
      "\n",
      "        [[[-0.1333,  0.2925,  0.3024],\n",
      "          [ 0.1723,  0.2485, -0.1362],\n",
      "          [ 0.2606,  0.2478,  0.2045]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3300,  0.2562, -0.0886],\n",
      "          [ 0.0323, -0.2184, -0.1972],\n",
      "          [-0.2699, -0.2487,  0.1701]]],\n",
      "\n",
      "\n",
      "        [[[-0.2829, -0.2944,  0.1632],\n",
      "          [-0.1158,  0.1628, -0.3100],\n",
      "          [-0.2624, -0.2495,  0.2075]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2126, -0.2177,  0.2864],\n",
      "          [ 0.2696, -0.2213, -0.0759],\n",
      "          [-0.1367, -0.0173, -0.3207]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1713, -0.2072, -0.2175],\n",
      "          [-0.2549,  0.1511,  0.0396],\n",
      "          [-0.0510,  0.1516, -0.1210]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2608, -0.3000, -0.0085],\n",
      "          [ 0.2618, -0.2118, -0.1199],\n",
      "          [-0.0900, -0.2643,  0.1767]]]], device='cuda:0')  B=tensor([[[[ 0.0031,  0.5474,  0.5704],\n",
      "          [ 0.1791, -0.2250,  0.3522],\n",
      "          [-0.5867, -0.6649, -0.5512]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1075, -0.3219, -0.3274],\n",
      "          [-0.2213, -0.2480,  0.0729],\n",
      "          [-0.2608, -0.0795,  0.0179]]],\n",
      "\n",
      "\n",
      "        [[[-0.4190,  0.0680, -0.5698],\n",
      "          [-0.2391, -0.2736, -0.3378],\n",
      "          [-0.7619, -0.1515, -0.2990]]],\n",
      "\n",
      "\n",
      "        [[[-0.7152, -0.1388,  0.3510],\n",
      "          [-0.0118, -0.4855, -0.3220],\n",
      "          [ 0.5355, -0.1182,  0.3063]]],\n",
      "\n",
      "\n",
      "        [[[-0.0260, -0.0337, -0.1510],\n",
      "          [ 0.0031,  0.0980,  0.0381],\n",
      "          [ 0.1374,  0.2043,  0.2056]]],\n",
      "\n",
      "\n",
      "        [[[-0.2404,  0.0088, -0.3479],\n",
      "          [ 0.0080, -0.4310,  0.0053],\n",
      "          [-0.2703,  0.0436, -0.3913]]],\n",
      "\n",
      "\n",
      "        [[[-0.1129, -0.4042, -0.3487],\n",
      "          [-0.3036, -0.4477, -0.3726],\n",
      "          [-0.3745, -0.2718, -0.2236]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1121, -0.2034, -0.5197],\n",
      "          [-0.1370, -0.1567, -0.3543],\n",
      "          [ 0.5711,  0.4284,  0.3829]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3001,  0.2372,  0.0345],\n",
      "          [ 0.0955, -0.3682, -0.0675],\n",
      "          [-0.3146, -0.0044, -0.3435]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3391,  0.3259, -0.2552],\n",
      "          [ 0.4193, -0.4615, -0.3538],\n",
      "          [-0.1310, -0.0014, -0.0631]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0416,  0.1601, -0.1252],\n",
      "          [ 0.4141, -0.1363,  0.1941],\n",
      "          [-0.1379, -0.5000, -0.1373]]],\n",
      "\n",
      "\n",
      "        [[[-0.3092, -0.2434, -0.3507],\n",
      "          [-0.0223, -0.2734, -0.7380],\n",
      "          [-0.2471, -0.1489, -0.3075]]],\n",
      "\n",
      "\n",
      "        [[[-0.2814, -0.2258,  0.1757],\n",
      "          [ 0.2317, -0.1895, -0.0178],\n",
      "          [-0.2648,  0.2810, -0.2121]]],\n",
      "\n",
      "\n",
      "        [[[-0.2571, -0.3222, -0.2580],\n",
      "          [-0.4746, -0.1026, -0.4818],\n",
      "          [-0.4065, -0.3798, -0.3984]]],\n",
      "\n",
      "\n",
      "        [[[-0.6036, -0.5105, -0.3219],\n",
      "          [ 0.3363,  0.1861,  0.2516],\n",
      "          [ 0.2756, -0.1963,  0.1165]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1082,  0.1202, -0.1486],\n",
      "          [ 0.1377, -0.0114, -0.4270],\n",
      "          [ 0.1172, -0.3091, -0.5163]]],\n",
      "\n",
      "\n",
      "        [[[-0.3625, -0.5709, -0.0218],\n",
      "          [-0.1993, -0.5500, -0.3137],\n",
      "          [-0.2602, -0.0128, -0.5425]]],\n",
      "\n",
      "\n",
      "        [[[-0.4771,  0.2861,  0.1960],\n",
      "          [-0.0997,  0.4185,  0.1264],\n",
      "          [ 0.1408,  0.3689, -0.2719]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3374, -0.0021, -0.7621],\n",
      "          [ 0.2440, -0.2529, -0.5228],\n",
      "          [ 0.0615, -0.2060, -0.6135]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0828, -0.2431,  0.1034],\n",
      "          [-0.0520,  0.1805,  0.3154],\n",
      "          [-0.3294, -0.0939,  0.2643]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0565, -0.6554, -0.5107],\n",
      "          [-0.0186, -0.5761, -0.0592],\n",
      "          [-0.5773, -0.3098, -0.3743]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0998,  0.3593,  0.1434],\n",
      "          [-0.4838,  0.1839,  0.3134],\n",
      "          [-0.4951, -0.3309, -0.2737]]],\n",
      "\n",
      "\n",
      "        [[[-0.3686, -0.3766,  0.1634],\n",
      "          [-0.6118,  0.2065,  0.2382],\n",
      "          [-0.5720, -0.0193,  0.4937]]],\n",
      "\n",
      "\n",
      "        [[[-0.3723, -0.0382,  0.1623],\n",
      "          [-0.0462, -0.2828,  0.4245],\n",
      "          [-0.3833,  0.1641,  0.2046]]],\n",
      "\n",
      "\n",
      "        [[[-0.1025,  0.0550, -0.0628],\n",
      "          [-0.5534, -0.2627,  0.3145],\n",
      "          [ 0.0836,  0.2574,  0.3024]]],\n",
      "\n",
      "\n",
      "        [[[-0.0742, -0.1464,  0.0143],\n",
      "          [-0.3591, -0.4701,  0.4034],\n",
      "          [-0.2967, -0.2772,  0.2745]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1270,  0.3825,  0.3093],\n",
      "          [-0.5110, -0.3392,  0.2692],\n",
      "          [-0.4414, -0.2136,  0.0945]]],\n",
      "\n",
      "\n",
      "        [[[-0.1121, -0.3828, -0.5413],\n",
      "          [-0.0041,  0.1067,  0.1480],\n",
      "          [ 0.2209,  0.2647,  0.1220]]],\n",
      "\n",
      "\n",
      "        [[[-0.0737, -0.0466, -0.2573],\n",
      "          [-0.0185,  0.0779,  0.0503],\n",
      "          [ 0.1694, -0.1940, -0.3059]]],\n",
      "\n",
      "\n",
      "        [[[-0.6717, -0.3807, -0.4838],\n",
      "          [-0.1382,  0.0474,  0.3781],\n",
      "          [ 0.3331,  0.1636,  0.4196]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047, -0.2782,  0.0203],\n",
      "          [-0.2499,  0.2181, -0.2706],\n",
      "          [-0.3014, -0.0567, -0.0241]]],\n",
      "\n",
      "\n",
      "        [[[-0.0856,  0.2659, -0.0105],\n",
      "          [ 0.3897,  0.3974, -0.1388],\n",
      "          [-0.4101, -0.4450, -0.2202]]]], device='cuda:0')\n",
      "A=tensor([ 0.2416, -0.0280, -0.0822,  0.1912,  0.2308,  0.2458, -0.1143, -0.3040,\n",
      "         0.1291,  0.0608,  0.3047,  0.2940,  0.2336,  0.1367,  0.2119, -0.1995,\n",
      "        -0.1696,  0.1018,  0.1788, -0.0762,  0.3265,  0.0449, -0.2274, -0.2615,\n",
      "         0.2862, -0.0409,  0.2338,  0.0708, -0.2189, -0.1463, -0.3166,  0.1782],\n",
      "       device='cuda:0')  B=tensor([-0.0210, -0.1877, -0.1280, -0.0137, -0.1782, -0.2845, -0.1474, -0.0715,\n",
      "        -0.1326, -0.0710, -0.0502, -0.1758, -0.3210, -0.1151, -0.0200, -0.2166,\n",
      "        -0.1371, -0.2916, -0.1614, -0.1380, -0.2029, -0.1427, -0.0156, -0.1014,\n",
      "         0.0030, -0.0317, -0.0642, -0.1056, -0.3585, -0.2628, -0.2409, -0.1116],\n",
      "       device='cuda:0')\n",
      "A=tensor([[-1.3208e-02, -1.0922e-02, -3.8258e-03,  ...,  7.1197e-03,\n",
      "         -1.3849e-03, -1.2774e-02],\n",
      "        [-9.4095e-03, -5.6484e-03, -1.0119e-02,  ...,  1.1790e-02,\n",
      "         -4.0880e-03, -1.3059e-02],\n",
      "        [-9.7451e-03, -8.1520e-03,  1.1568e-02,  ..., -9.0087e-03,\n",
      "          9.9514e-03, -1.0709e-02],\n",
      "        ...,\n",
      "        [ 7.7995e-03, -8.3304e-03, -3.2919e-05,  ..., -1.9245e-03,\n",
      "         -3.8079e-03, -2.4050e-03],\n",
      "        [-6.7186e-03, -1.2413e-03,  5.2673e-03,  ...,  1.3070e-02,\n",
      "         -1.8146e-03,  1.0449e-02],\n",
      "        [-5.4518e-03, -9.1620e-03, -7.5541e-03,  ..., -3.2843e-03,\n",
      "         -1.9106e-04,  1.0901e-02]], device='cuda:0')  B=tensor([[-0.0039, -0.0065, -0.0116,  ..., -0.0708, -0.0563, -0.0687],\n",
      "        [ 0.0034, -0.0116, -0.0117,  ..., -0.0455, -0.0680, -0.0553],\n",
      "        [-0.0928, -0.0854, -0.0896,  ..., -0.0923, -0.0982, -0.0886],\n",
      "        ...,\n",
      "        [ 0.0519,  0.0383,  0.0568,  ..., -0.1440, -0.0423, -0.0283],\n",
      "        [ 0.0095, -0.0030,  0.0122,  ..., -0.0532, -0.0498, -0.0596],\n",
      "        [-0.0105, -0.0072,  0.0050,  ..., -0.0643, -0.0705, -0.0700]],\n",
      "       device='cuda:0')\n",
      "A=tensor([ 2.0401e-03, -9.9762e-03, -6.6611e-03, -1.3405e-02,  2.8944e-04,\n",
      "         1.3256e-02, -1.2505e-02, -9.1721e-03, -1.0649e-02, -3.3006e-05,\n",
      "        -6.7446e-03,  4.8501e-03,  2.5182e-03,  1.2660e-02,  2.7151e-03,\n",
      "        -1.2294e-02,  4.3444e-03, -9.9130e-03, -1.0445e-03,  5.0037e-03,\n",
      "         1.0121e-02, -1.1531e-02, -9.4763e-03,  3.9552e-03, -7.2214e-03,\n",
      "        -9.1533e-03,  1.1072e-04, -1.9388e-03,  6.7632e-03,  1.5838e-03,\n",
      "        -4.9483e-04, -4.6070e-03,  7.3829e-03,  9.5492e-03,  1.1154e-02,\n",
      "         6.6784e-03,  1.1079e-02, -3.0032e-03,  4.9019e-03,  8.0611e-03,\n",
      "        -2.6142e-03, -5.5404e-03,  1.2918e-02,  7.6457e-03, -1.1142e-02,\n",
      "        -1.1444e-03,  4.9786e-03, -7.4820e-03, -3.3946e-03, -8.1752e-03,\n",
      "        -5.0306e-03,  5.8266e-03,  3.9945e-03, -2.5222e-05,  6.9198e-03,\n",
      "        -7.8378e-03, -3.1485e-03,  9.3176e-03,  1.2612e-02,  9.4611e-03,\n",
      "         1.1714e-03,  4.7890e-03,  2.5923e-03, -4.4633e-03,  5.5655e-03,\n",
      "        -1.2867e-02,  8.8648e-03,  3.4125e-03,  1.1675e-02,  1.0869e-02,\n",
      "         1.9260e-03, -5.8157e-03, -1.1782e-02,  1.2546e-02, -2.3739e-03,\n",
      "        -4.1532e-03, -1.3142e-02,  1.2186e-02,  3.7268e-04, -1.2179e-02,\n",
      "         2.5797e-03,  7.8018e-03, -9.0325e-03, -3.3081e-03, -2.8357e-03,\n",
      "         8.6724e-03, -1.7681e-03,  5.4239e-03, -1.2122e-02, -1.4466e-03,\n",
      "        -1.2866e-02,  1.0285e-02,  9.1090e-03,  1.3189e-02,  8.3507e-03,\n",
      "         7.3303e-03,  2.5674e-03, -6.5915e-03,  4.2709e-03, -3.3128e-04,\n",
      "        -3.5298e-04,  3.8720e-03,  8.8261e-03,  7.7565e-03, -1.1074e-02,\n",
      "         8.4922e-03,  1.1912e-02,  7.7642e-04, -6.5452e-03, -5.3251e-03,\n",
      "        -6.3669e-04,  3.0239e-03, -7.0208e-03, -2.5489e-03,  1.1505e-02,\n",
      "         4.1045e-03,  2.8703e-04, -5.2142e-03, -7.9807e-03,  1.0431e-02,\n",
      "        -7.4227e-03, -9.6784e-03,  9.7169e-03,  7.4519e-03, -7.9306e-03,\n",
      "         1.7295e-03,  8.9626e-03, -2.5448e-03, -4.7890e-03,  1.3501e-02,\n",
      "        -1.3082e-02, -2.6652e-03,  3.1857e-03, -3.9793e-03,  2.5986e-03,\n",
      "        -9.3881e-03,  1.3362e-02, -3.5543e-03, -1.1591e-02, -5.5257e-03,\n",
      "        -8.3589e-03, -8.7353e-03, -4.3169e-03,  7.6796e-04, -3.1661e-03,\n",
      "        -1.2695e-02, -2.1246e-03,  4.4872e-03,  9.6738e-03, -6.9634e-03,\n",
      "         1.1396e-02,  1.0921e-02, -3.0752e-03,  1.1406e-02,  4.0057e-03,\n",
      "         7.0173e-03,  8.5732e-03,  3.5945e-03, -8.5629e-03,  1.2760e-02,\n",
      "        -1.1143e-02, -8.6276e-03, -6.5987e-03,  1.0293e-02, -1.1559e-02,\n",
      "        -4.2387e-03, -1.2809e-02,  6.8108e-03,  8.7709e-03, -1.3036e-02,\n",
      "         5.5092e-03, -6.6976e-03,  7.7165e-03,  1.3523e-02, -8.7955e-03,\n",
      "         7.1180e-03,  5.1131e-03, -1.1276e-02,  4.9773e-03, -1.0072e-02,\n",
      "         7.0372e-03, -1.8028e-03, -1.2714e-02, -5.5855e-03, -6.3170e-03,\n",
      "        -6.3565e-03, -8.7336e-04, -3.9543e-03, -1.0733e-02,  1.5197e-03,\n",
      "        -1.1046e-02, -7.0480e-03, -3.3378e-03,  6.8558e-03, -5.0364e-03,\n",
      "        -8.7326e-03,  3.7192e-03, -8.7067e-03, -4.9792e-03, -1.7135e-03,\n",
      "         1.2202e-02, -3.7342e-03,  8.8101e-03, -1.1199e-02, -8.6011e-03,\n",
      "         9.0741e-03,  1.0706e-02, -9.1954e-03,  1.0165e-02,  5.1330e-03,\n",
      "         2.2555e-03, -5.3060e-04,  6.6240e-03,  4.5400e-03,  1.1328e-02,\n",
      "        -1.0156e-02,  1.1527e-02, -2.4784e-04,  1.0462e-02,  2.9286e-03,\n",
      "         1.0387e-02,  6.3953e-03,  1.2757e-02, -8.4321e-03, -4.0200e-03,\n",
      "        -1.5944e-03,  8.9687e-04,  4.8596e-03,  5.1323e-03,  1.2577e-02,\n",
      "        -4.6096e-04, -1.3488e-02,  8.3087e-03,  8.6023e-03,  3.0643e-03,\n",
      "        -1.2832e-02, -1.2909e-02,  6.0613e-03,  5.2569e-03, -4.2060e-03,\n",
      "         7.1096e-03, -1.3332e-02,  1.0643e-02,  1.2057e-02, -1.3288e-02,\n",
      "        -4.9080e-03,  8.5267e-03,  8.4040e-03, -7.9961e-03, -1.4204e-03,\n",
      "        -2.6537e-03,  1.2602e-02,  7.7233e-04,  7.2902e-03,  9.7218e-03,\n",
      "         1.8476e-03], device='cuda:0')  B=tensor([-0.1077, -0.0453,  0.0141, -0.0746, -0.0714, -0.0699, -0.0507, -0.0057,\n",
      "        -0.0338, -0.0516, -0.0517,  0.0224, -0.0339, -0.0570, -0.0564, -0.0981,\n",
      "         0.0347, -0.0370, -0.0481, -0.0572, -0.0573, -0.0514, -0.0572, -0.0224,\n",
      "        -0.0332,  0.0435,  0.0901, -0.0462, -0.0476, -0.0685, -0.0643, -0.0439,\n",
      "         0.0135, -0.0556, -0.0496, -0.0399, -0.0904, -0.0490,  0.0198, -0.0618,\n",
      "        -0.0615, -0.0568, -0.0597, -0.0475, -0.0170, -0.0283, -0.0684, -0.0583,\n",
      "        -0.0465, -0.0533, -0.0367, -0.0291, -0.0702, -0.0533, -0.0646, -0.0506,\n",
      "        -0.0487, -0.0387, -0.0311,  0.1047, -0.0512, -0.0409, -0.0568, -0.0790,\n",
      "        -0.0702, -0.0707, -0.0725, -0.0625, -0.0710, -0.0661, -0.0651, -0.1349,\n",
      "        -0.0702, -0.0510, -0.0731, -0.0100, -0.0514, -0.0549, -0.0674, -0.0329,\n",
      "        -0.0452, -0.1161,  0.0034, -0.0613, -0.0283, -0.0680, -0.0656, -0.0597,\n",
      "        -0.0859, -0.0668, -0.0033, -0.0382, -0.1540,  0.0606, -0.0900,  0.0597,\n",
      "        -0.0498,  0.0409, -0.0784, -0.0517, -0.0535, -0.0612, -0.0538, -0.0557,\n",
      "        -0.0557, -0.1545,  0.0743, -0.0089,  0.0346, -0.0550, -0.0747, -0.0565,\n",
      "        -0.0483, -0.0873, -0.0469, -0.0353, -0.0653, -0.0470, -0.1065, -0.0510,\n",
      "        -0.0426, -0.1143, -0.0479, -0.0648, -0.0539, -0.1000, -0.0240, -0.0657,\n",
      "        -0.1609, -0.1302, -0.0562, -0.1115, -0.0581, -0.0505,  0.0820, -0.0684,\n",
      "        -0.1228,  0.0019, -0.0648, -0.0625, -0.0468, -0.0389,  0.0031, -0.0596,\n",
      "        -0.0715, -0.1487, -0.0521, -0.0565, -0.0345, -0.0667, -0.0538, -0.0466,\n",
      "        -0.0702, -0.0548, -0.0708, -0.0480, -0.1407, -0.1139, -0.0659, -0.0476,\n",
      "        -0.0966, -0.0435, -0.0690, -0.0597, -0.0678, -0.0636, -0.0183, -0.0571,\n",
      "        -0.0644, -0.0476, -0.1517, -0.0513, -0.0605, -0.1182, -0.0511, -0.0499,\n",
      "        -0.0571, -0.0605, -0.0407, -0.0488,  0.0224, -0.1190, -0.0629, -0.0445,\n",
      "        -0.0914, -0.0155, -0.0652, -0.0675, -0.0474, -0.0799, -0.0608, -0.0622,\n",
      "         0.0486, -0.0414, -0.0914,  0.0332, -0.0822, -0.0784, -0.0531, -0.0541,\n",
      "        -0.0366,  0.0027, -0.0520, -0.0498, -0.0347, -0.0358, -0.0667, -0.0517,\n",
      "        -0.0332, -0.0711,  0.0566, -0.0697, -0.0647, -0.0602, -0.0677, -0.0187,\n",
      "         0.0180, -0.0357, -0.0519,  0.0195, -0.0467, -0.0567, -0.0248, -0.0745,\n",
      "        -0.0500, -0.0655,  0.0422, -0.0586, -0.1124, -0.0671, -0.0533, -0.0637,\n",
      "        -0.1118, -0.0297, -0.0470, -0.0470, -0.0683, -0.0466, -0.1045, -0.0543,\n",
      "         0.1379, -0.0590, -0.0988, -0.0484, -0.0566, -0.0531, -0.0749, -0.0401,\n",
      "        -0.0173, -0.0572, -0.0450, -0.0283, -0.0313,  0.0173, -0.0744, -0.0496],\n",
      "       device='cuda:0')\n",
      "A=tensor([[ 0.0051,  0.0079,  0.0568,  ..., -0.0468,  0.0578, -0.0309],\n",
      "        [-0.0258,  0.0440,  0.0421,  ...,  0.0553,  0.0582, -0.0561],\n",
      "        [ 0.0580, -0.0356,  0.0099,  ...,  0.0474, -0.0366, -0.0393],\n",
      "        ...,\n",
      "        [ 0.0145, -0.0266, -0.0501,  ...,  0.0177, -0.0243, -0.0218],\n",
      "        [-0.0116, -0.0119,  0.0368,  ...,  0.0073, -0.0008, -0.0476],\n",
      "        [ 0.0391,  0.0010,  0.0196,  ...,  0.0048,  0.0088,  0.0570]],\n",
      "       device='cuda:0')  B=tensor([[-0.0448,  0.0111, -0.1478,  ..., -0.2055, -0.0148,  0.0583],\n",
      "        [-0.0634,  0.0882, -0.1539,  ..., -0.3187,  0.0293, -0.1022],\n",
      "        [-0.0082,  0.0222, -0.1286,  ...,  0.0758,  0.0116, -0.0646],\n",
      "        ...,\n",
      "        [ 0.1307, -0.0183, -0.0911,  ...,  0.1242, -0.0871,  0.0093],\n",
      "        [-0.0744,  0.0086, -0.1516,  ...,  0.0142, -0.0573, -0.0313],\n",
      "        [ 0.0637,  0.0254, -0.1123,  ..., -0.1771, -0.0962,  0.0467]],\n",
      "       device='cuda:0')\n",
      "A=tensor([-0.0056, -0.0033, -0.0443,  0.0443, -0.0135, -0.0173, -0.0592, -0.0094,\n",
      "         0.0561,  0.0372], device='cuda:0')  B=tensor([-0.0351,  0.0843,  0.0643, -0.0466,  0.0167,  0.1036, -0.1061,  0.0104,\n",
      "        -0.0683, -0.1537], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sdA = model.state_dict()\n",
    "sdB = model_tmp.state_dict()\n",
    "\n",
    "# Average all parameters\n",
    "for key in sdA:\n",
    "    print(\"A={}  B={}\".format(sdA[key],sdB[key]))\n",
    "#sdB[key] = (sdB[key] + sdA[key]) / 2.\n",
    "\n",
    "# Recreate model and load averaged state_dict (or use modelA/B)\n",
    "# model = nn.Linear(1, 1)\n",
    "# model.load_state_dict(sdB)\n",
    "\n",
    "# model_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if model in cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "        \n",
    "model.conv1 = th.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "batch = th.rand(4, 1, 224, 224) # in the NCHW format\n",
    "model(batch).size()\n",
    "\n",
    "\n",
    "in_feature =  model.fc.in_features\n",
    "model.fc = nn.Linear(in_feature, NUM_CLASSES)  # ch\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
